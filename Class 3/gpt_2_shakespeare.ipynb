{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "d586e686-8224-454f-cddf-9a49fbbdeb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort (from gpt-2-simple)\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24559 sha256=8dd46c263e7d8f44dd2452937fbd82a3a5a3b86371178b18a387b7b5a8ef297a\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/6a/fe/10d3223f78d1ac3e4c83bb4c5e2d28dfb1789c2fb4cc7ea8d0\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "f47751fe-acd5-4909-c30d-aaa402754109"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 502Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 935kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 127Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:36, 13.5Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 724Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.40Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 1.41Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "f4f225a8-1e3e-4062-9f78-dcbf7169dcc3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "6e793524-0e49-43d6-ca54-da167acc9475"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "NASA: Yes, there is a second Earth. The second Earth is the only Earth that existed before the solar system's formation in our solar system. It is the only Earth that the solar system has ever known before and it is the only\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "51f9ff97-287d-4c94-ab61-7c27b14fb7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:03--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.95.189, 54.231.169.144, 52.217.50.6, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.95.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-21 15:19:03 (4.21 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "19fa5fe5-7ccd-43b5-da72-81c36df0aedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 147MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "b59755af-4d8a-4592-c71a-bee603432b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-21 15:19:13 (19.9 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "34ac2399-1ea3-431f-d601-31329092c787"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 7.18] loss=3.40 avg=3.40\n",
            "[2 | 9.42] loss=3.59 avg=3.50\n",
            "[3 | 11.66] loss=3.53 avg=3.51\n",
            "[4 | 13.91] loss=3.46 avg=3.50\n",
            "[5 | 16.18] loss=3.42 avg=3.48\n",
            "[6 | 18.45] loss=3.23 avg=3.44\n",
            "[7 | 20.74] loss=3.29 avg=3.42\n",
            "[8 | 23.03] loss=3.39 avg=3.41\n",
            "[9 | 25.34] loss=3.20 avg=3.39\n",
            "[10 | 27.68] loss=3.29 avg=3.38\n",
            "[11 | 30.02] loss=3.40 avg=3.38\n",
            "[12 | 32.38] loss=3.24 avg=3.37\n",
            "[13 | 34.75] loss=3.13 avg=3.35\n",
            "[14 | 37.16] loss=3.29 avg=3.34\n",
            "[15 | 39.58] loss=3.15 avg=3.33\n",
            "[16 | 42.03] loss=3.09 avg=3.31\n",
            "[17 | 44.48] loss=3.11 avg=3.30\n",
            "[18 | 46.94] loss=3.19 avg=3.29\n",
            "[19 | 49.39] loss=3.07 avg=3.28\n",
            "[20 | 51.84] loss=3.10 avg=3.27\n",
            "[21 | 54.27] loss=2.98 avg=3.26\n",
            "[22 | 56.70] loss=3.10 avg=3.25\n",
            "[23 | 59.08] loss=3.09 avg=3.24\n",
            "[24 | 61.45] loss=3.05 avg=3.23\n",
            "[25 | 63.81] loss=3.09 avg=3.23\n",
            "[26 | 66.16] loss=3.06 avg=3.22\n",
            "[27 | 68.50] loss=2.98 avg=3.21\n",
            "[28 | 70.84] loss=2.98 avg=3.20\n",
            "[29 | 73.15] loss=3.33 avg=3.20\n",
            "[30 | 75.48] loss=3.20 avg=3.20\n",
            "[31 | 77.78] loss=3.08 avg=3.20\n",
            "[32 | 80.09] loss=3.12 avg=3.20\n",
            "[33 | 82.39] loss=3.11 avg=3.19\n",
            "[34 | 84.70] loss=3.01 avg=3.19\n",
            "[35 | 87.00] loss=3.12 avg=3.18\n",
            "[36 | 89.32] loss=3.06 avg=3.18\n",
            "[37 | 91.62] loss=3.00 avg=3.17\n",
            "[38 | 93.93] loss=2.96 avg=3.17\n",
            "[39 | 96.24] loss=3.10 avg=3.17\n",
            "[40 | 98.56] loss=3.02 avg=3.16\n",
            "[41 | 100.88] loss=3.15 avg=3.16\n",
            "[42 | 103.20] loss=2.98 avg=3.16\n",
            "[43 | 105.54] loss=3.00 avg=3.15\n",
            "[44 | 107.89] loss=3.02 avg=3.15\n",
            "[45 | 110.24] loss=2.90 avg=3.14\n",
            "[46 | 112.60] loss=2.95 avg=3.14\n",
            "[47 | 114.95] loss=2.90 avg=3.13\n",
            "[48 | 117.30] loss=2.98 avg=3.13\n",
            "[49 | 119.67] loss=2.98 avg=3.12\n",
            "[50 | 122.06] loss=2.86 avg=3.12\n",
            "[51 | 124.43] loss=2.87 avg=3.11\n",
            "[52 | 126.80] loss=3.11 avg=3.11\n",
            "[53 | 129.18] loss=3.05 avg=3.11\n",
            "[54 | 131.55] loss=2.91 avg=3.10\n",
            "[55 | 133.92] loss=3.09 avg=3.10\n",
            "[56 | 136.30] loss=3.09 avg=3.10\n",
            "[57 | 138.67] loss=2.89 avg=3.10\n",
            "[58 | 141.04] loss=2.91 avg=3.09\n",
            "[59 | 143.40] loss=3.01 avg=3.09\n",
            "[60 | 145.75] loss=2.93 avg=3.09\n",
            "[61 | 148.10] loss=3.04 avg=3.09\n",
            "[62 | 150.46] loss=2.90 avg=3.08\n",
            "[63 | 152.79] loss=2.92 avg=3.08\n",
            "[64 | 155.14] loss=3.02 avg=3.08\n",
            "[65 | 157.48] loss=2.75 avg=3.07\n",
            "[66 | 159.84] loss=2.81 avg=3.07\n",
            "[67 | 162.18] loss=2.83 avg=3.06\n",
            "[68 | 164.52] loss=2.88 avg=3.06\n",
            "[69 | 166.86] loss=2.83 avg=3.05\n",
            "[70 | 169.19] loss=2.82 avg=3.05\n",
            "[71 | 171.53] loss=2.79 avg=3.04\n",
            "[72 | 173.88] loss=2.95 avg=3.04\n",
            "[73 | 176.21] loss=2.88 avg=3.04\n",
            "[74 | 178.54] loss=2.79 avg=3.03\n",
            "[75 | 180.88] loss=2.93 avg=3.03\n",
            "[76 | 183.21] loss=2.92 avg=3.03\n",
            "[77 | 185.57] loss=2.87 avg=3.03\n",
            "[78 | 187.91] loss=2.84 avg=3.02\n",
            "[79 | 190.27] loss=2.87 avg=3.02\n",
            "[80 | 192.61] loss=2.98 avg=3.02\n",
            "[81 | 194.96] loss=2.98 avg=3.02\n",
            "[82 | 197.32] loss=2.92 avg=3.02\n",
            "[83 | 199.66] loss=2.80 avg=3.01\n",
            "[84 | 202.01] loss=2.82 avg=3.01\n",
            "[85 | 204.35] loss=2.83 avg=3.01\n",
            "[86 | 206.71] loss=2.85 avg=3.00\n",
            "[87 | 209.06] loss=2.86 avg=3.00\n",
            "[88 | 211.43] loss=2.95 avg=3.00\n",
            "[89 | 213.78] loss=2.94 avg=3.00\n",
            "[90 | 216.13] loss=3.04 avg=3.00\n",
            "[91 | 218.48] loss=2.81 avg=3.00\n",
            "[92 | 220.84] loss=2.88 avg=2.99\n",
            "[93 | 223.19] loss=2.90 avg=2.99\n",
            "[94 | 225.54] loss=3.03 avg=2.99\n",
            "[95 | 227.89] loss=2.80 avg=2.99\n",
            "[96 | 230.25] loss=2.84 avg=2.99\n",
            "[97 | 232.60] loss=2.94 avg=2.99\n",
            "[98 | 234.96] loss=3.02 avg=2.99\n",
            "[99 | 237.30] loss=2.81 avg=2.99\n",
            "[100 | 239.65] loss=2.80 avg=2.98\n",
            "======== SAMPLE 1 ========\n",
            " too. \n",
            "Page 38\n",
            "\n",
            "I thought I was going to die with someone else. I had no notion that it was going to happen. \n",
            "I did not see it coming. They seemed a pretty great place, though. It seemed the only good \n",
            "way \n",
            "to go to \n",
            "my stomach. They were not the first place I'd ever seen. \n",
            "I went down one night as a child to see Lady Cleopatra riding a horse on a mare she had stolen \n",
            "from the castle. She had her breasts and her arms full of pig droppings, she was a whore, and \n",
            "she had a lot of silver and gold. She looked a hell of a girl to me. I went by the \n",
            "Page 39\n",
            "\n",
            "name of Lady Cleopatra, and the woman I saw, who looked about twenty years older than I was. \n",
            "Lady Cleopatra had a white face and brown hair of a ponytail that seemed to grow ever thinner \n",
            "for some reason. There were no lips, although one ponytail was still a ponytail. She was the \n",
            "mother of her four children. They were each named after her. There was one child born between \n",
            "the years of four and six years, another born when they were in their late teens, the third born when \n",
            "they were in their late 30s, and then the third child, and finally finally the fourth, and the fifth child. \n",
            "\"Oh, my lord,\" the king said. \n",
            "\"It's no great shame, for my daughter has a pretty face,\" I said. \"How does the man know?\" \n",
            "\"He's old, and he always is,\" the king said. \"He should make you a gift of my arms in a moment. Your arms are more \n",
            "complicated than swords, of course, but do you not feel as much pain as a man growing up? I have no \n",
            "doubt in my heart that you do.\" He turned round, his long black hair gathering in the wind and shining off his chest, \n",
            "a grey, and a scarred smile. \"I remember you, I remember you.\" His voice was deep and heavy and \n",
            "shaggy, like a heavy weight on a horse. \"I remember what you said, I recall you.\" \n",
            "\"Lord Stark will see that,\" I said. I felt the need to remind him he was right. \"I must bring \n",
            "you back. A fortnight ago, a great beast of a woman had been found in our king's yard.\" \n",
            "His arms were too massive for my size. My legs had grown huge for my weight. If those hands were any help, she \n",
            "was a monster. Oh, I hated it. A monster of a beast. I tried to stop myself, but my leg didn't \n",
            "work. I could hear the moans of a thousand birds that fled the street. \n",
            "I thought about how much pain this little monster had inflicted on my wife, a monster of a monster. \"You must \n",
            "be going away before I'm done with you, are you? If you take my arms, I will need to give you \n",
            "your new ones.\" \n",
            "That was wrong. There was no need , nor should there be. I had no choice, and my wife had nothing to \n",
            "help me stop, because that would kill me. \n",
            "\"My love,\" he said. \"You are the only good thing in this world. Do you know how to make a gift? You'll \n",
            "know what I'm talking about, my lord.\" \n",
            "\"It must be more than a gift,\" I said. \"You must love my son, and I hope that your \n",
            "warrants will be good, too. I must work as hard as I can to make them as healthy as I can. This will be as \n",
            "well as any gift you make.\" \n",
            "\"Thank you, my lord,\" Lord Stark said. \"I have no secret, I did it long ago, I truly ought to make \n",
            "it.\" He shook his chair awkwardly. \n",
            "\"We do all have secret things, are you, my man?\" I asked him. If I had more than two arms, I was sure \n",
            "I could take the Iron Throne, as long as there were servants, and I did . . . and that was the least of my \n",
            "secret duties.\" \n",
            "By now his eyes were on me, as if in the twilight. \n",
            "I could tell that it was no secret he liked me. I heard the voice of a young girl in the high-roofed hall. \"Lying \n",
            "Page 39\n",
            "\n",
            "there,\" I told her, and in the quiet of the highroad of the city. It was the only thing he'd ever done \n",
            "that would make him forget himself. \n",
            "He did not seem to see her anymore\n",
            "\n",
            "[101 | 254.22] loss=3.05 avg=2.98\n",
            "[102 | 256.57] loss=2.80 avg=2.98\n",
            "[103 | 258.93] loss=2.94 avg=2.98\n",
            "[104 | 261.27] loss=2.85 avg=2.98\n",
            "[105 | 263.62] loss=2.87 avg=2.98\n",
            "[106 | 265.97] loss=3.01 avg=2.98\n",
            "[107 | 268.32] loss=2.82 avg=2.97\n",
            "[108 | 270.67] loss=2.78 avg=2.97\n",
            "[109 | 273.01] loss=2.70 avg=2.97\n",
            "[110 | 275.36] loss=2.63 avg=2.96\n",
            "[111 | 277.71] loss=2.81 avg=2.96\n",
            "[112 | 280.06] loss=2.85 avg=2.96\n",
            "[113 | 282.41] loss=2.70 avg=2.95\n",
            "[114 | 284.76] loss=2.79 avg=2.95\n",
            "[115 | 287.10] loss=2.80 avg=2.95\n",
            "[116 | 289.45] loss=2.76 avg=2.95\n",
            "[117 | 291.80] loss=2.63 avg=2.94\n",
            "[118 | 294.15] loss=2.63 avg=2.94\n",
            "[119 | 296.50] loss=2.92 avg=2.94\n",
            "[120 | 298.85] loss=2.95 avg=2.94\n",
            "[121 | 301.19] loss=2.94 avg=2.94\n",
            "[122 | 303.54] loss=2.71 avg=2.93\n",
            "[123 | 305.89] loss=2.88 avg=2.93\n",
            "[124 | 308.24] loss=2.72 avg=2.93\n",
            "[125 | 310.59] loss=2.83 avg=2.93\n",
            "[126 | 312.94] loss=2.80 avg=2.93\n",
            "[127 | 315.29] loss=2.77 avg=2.93\n",
            "[128 | 317.64] loss=2.76 avg=2.92\n",
            "[129 | 319.99] loss=2.75 avg=2.92\n",
            "[130 | 322.34] loss=2.78 avg=2.92\n",
            "[131 | 324.69] loss=2.75 avg=2.92\n",
            "[132 | 327.04] loss=2.75 avg=2.91\n",
            "[133 | 329.40] loss=2.62 avg=2.91\n",
            "[134 | 331.75] loss=2.80 avg=2.91\n",
            "[135 | 334.09] loss=2.82 avg=2.91\n",
            "[136 | 336.44] loss=2.70 avg=2.91\n",
            "[137 | 338.78] loss=2.71 avg=2.90\n",
            "[138 | 341.14] loss=2.60 avg=2.90\n",
            "[139 | 343.48] loss=2.56 avg=2.89\n",
            "[140 | 345.84] loss=3.03 avg=2.90\n",
            "[141 | 348.18] loss=2.70 avg=2.89\n",
            "[142 | 350.53] loss=2.66 avg=2.89\n",
            "[143 | 352.88] loss=2.85 avg=2.89\n",
            "[144 | 355.23] loss=2.91 avg=2.89\n",
            "[145 | 357.57] loss=2.67 avg=2.89\n",
            "[146 | 359.92] loss=2.76 avg=2.89\n",
            "[147 | 362.27] loss=2.68 avg=2.88\n",
            "[148 | 364.61] loss=2.70 avg=2.88\n",
            "[149 | 366.96] loss=2.65 avg=2.88\n",
            "[150 | 369.31] loss=2.67 avg=2.87\n",
            "[151 | 371.66] loss=2.51 avg=2.87\n",
            "[152 | 374.01] loss=2.73 avg=2.87\n",
            "[153 | 376.35] loss=2.50 avg=2.86\n",
            "[154 | 378.69] loss=2.36 avg=2.86\n",
            "[155 | 381.04] loss=2.68 avg=2.85\n",
            "[156 | 383.38] loss=2.73 avg=2.85\n",
            "[157 | 385.74] loss=2.73 avg=2.85\n",
            "[158 | 388.09] loss=2.64 avg=2.85\n",
            "[159 | 390.44] loss=2.52 avg=2.85\n",
            "[160 | 392.79] loss=2.54 avg=2.84\n",
            "[161 | 395.14] loss=2.64 avg=2.84\n",
            "[162 | 397.49] loss=2.76 avg=2.84\n",
            "[163 | 399.84] loss=2.61 avg=2.83\n",
            "[164 | 402.19] loss=2.54 avg=2.83\n",
            "[165 | 404.54] loss=2.56 avg=2.83\n",
            "[166 | 406.88] loss=2.54 avg=2.82\n",
            "[167 | 409.23] loss=2.64 avg=2.82\n",
            "[168 | 411.58] loss=2.62 avg=2.82\n",
            "[169 | 413.91] loss=2.87 avg=2.82\n",
            "[170 | 416.26] loss=2.70 avg=2.82\n",
            "[171 | 418.60] loss=2.57 avg=2.82\n",
            "[172 | 420.96] loss=2.68 avg=2.81\n",
            "[173 | 423.30] loss=2.54 avg=2.81\n",
            "[174 | 425.64] loss=2.80 avg=2.81\n",
            "[175 | 428.00] loss=2.61 avg=2.81\n",
            "[176 | 430.35] loss=2.63 avg=2.81\n",
            "[177 | 432.70] loss=2.79 avg=2.81\n",
            "[178 | 435.04] loss=2.85 avg=2.81\n",
            "[179 | 437.39] loss=2.66 avg=2.80\n",
            "[180 | 439.74] loss=2.70 avg=2.80\n",
            "[181 | 442.09] loss=2.58 avg=2.80\n",
            "[182 | 444.44] loss=2.67 avg=2.80\n",
            "[183 | 446.79] loss=2.76 avg=2.80\n",
            "[184 | 449.14] loss=2.59 avg=2.80\n",
            "[185 | 451.48] loss=2.67 avg=2.79\n",
            "[186 | 453.82] loss=2.61 avg=2.79\n",
            "[187 | 456.17] loss=2.53 avg=2.79\n",
            "[188 | 458.52] loss=2.73 avg=2.79\n",
            "[189 | 460.87] loss=2.41 avg=2.78\n",
            "[190 | 463.22] loss=2.71 avg=2.78\n",
            "[191 | 465.57] loss=2.63 avg=2.78\n",
            "[192 | 467.91] loss=2.53 avg=2.78\n",
            "[193 | 470.26] loss=2.70 avg=2.78\n",
            "[194 | 472.59] loss=2.58 avg=2.78\n",
            "[195 | 474.93] loss=2.60 avg=2.77\n",
            "[196 | 477.27] loss=2.43 avg=2.77\n",
            "[197 | 479.62] loss=2.46 avg=2.77\n",
            "[198 | 481.97] loss=2.54 avg=2.76\n",
            "[199 | 484.32] loss=2.60 avg=2.76\n",
            "[200 | 486.65] loss=2.39 avg=2.76\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "\"A boy like that . . .\" He looked at his own \n",
            "sideset, trying to make sense of it all. A young child . . . but one who knew how to talk, and \n",
            "who knew what the words meant . . . and yet this, it had only taken him so long. \"He \n",
            "was,\" he whispered. \"He'll be there by then.\" \n",
            "\"Do it, Robert. Put all your trust in that, and I'll keep it for you,\" Roy told Ned. \n",
            "\"Robert was only fourteen,\" Ned warned him. \"You'll never see a boy as young or strong as Eddard Stark.\" \n",
            "Robert laughed. Ned laughed, sure. They were friends now, before his age. Ned knew his brother had been \n",
            "banned a few times as well. When Dony sent him over to Dany's ward, they were alone, so it was all \n",
            "hard work to convince their other brothers that were wrong. Ned could feel the tears in his eyes, and Robert \n",
            "hearked back at his uncle, saying that he should have done it sooner. \"What do you think?\" \n",
            "\"They killed a boy, Ned?\" Ned asked, surprised. He never expected that. \n",
            "\"Did anyone do it?\" His uncle did not seem to agree. \"A man must have been strong enough to protect \n",
            "you.\" \n",
            "\"Robb, I thought I heard you say,\" Ned said. \"I thought you meant it . . .\" He had seen Robb the last \n",
            "time he looked, he knew it. The smile was almost black around Jaime Lannister's face. \"Don, \n",
            "I think I heard you,\" Ned said with a smile that felt even colder than that on the street. \n",
            "\"We were both watching you, you know,\" Stark said. \n",
            "\"Robb, I thought that was supposed to happen,\" Ned said, \"but I'd thought that it was worse. I don't know, \n",
            "did it always happen . . .\" \n",
            "The boy had grown a heartbeat, and for that Ned had known him to be, but the words were not \n",
            "coming. \"Robert was always my father's Hand, I never knew his father before, but the way he looked at me made me \n",
            "see that he was Robb's apprentice as much as any of my brothers. All the good things I learned at home \n",
            "have gone in vain. I hate it when anyone says you're too stupid to know what to do but how \n",
            "Do you feel if you say you didn't always do what I did.\" \n",
            "\"However.\" Robb said nothing. \"I'll be back and get you back,\" he said. \n",
            "\"As you say.\" Ned looked very sad on the road. \n",
            "\"Please, please,\" Stark urged him. \"Please.\" \n",
            "Robert never spoke. The last words Ned could make came from within his heart. \"Robert, please,\" he \n",
            "said, his voice hushed in his throat, \"I'm sorry.\" He looked sad. Robert's eyes flickered blue as he looked at his \n",
            "father. \"What are you going to do?\" \n",
            "\"Stop doing this, Stark, it makes me sick.\" Ned was shaking, and when he looked at the blood, they were \n",
            "each in the same room. \"Look at me, my lord. Look at me too, Robb.\" \n",
            "\"It does not matter, the girl was the stronger, I won't hurt her again.\" \n",
            "\"Robb won't even talk about things, don't tell him that. You took him for your brother, so he thinks \n",
            "you stole him.\" \n",
            "Robert's eyes opened. \"So you are . . . a bastard . . . what . . . I don't know how you got here, but \n",
            "once, I would have sworn that.\" The words were coming out of Robert's mouth like a knife thrust deep into \n",
            "his chest. He shook. \"Why do you ask? Why? Why?\" \n",
            "Robert looked at Ned for a moment, surprised. \"You don't ask, Ned.\" \n",
            "\"Why?\" The boy shook with laughter and anger. \"Why not?\" he demanded. \"You will not survive this. You will \n",
            "never survive. I am Robb Stark, not your father . . .\" \n",
            "\"You shall,\" Ned said defiantly. He was going to die of laughter. \"How dare you tell me this. You will be the \n",
            "last man I ever want to be. You will never see a boy like that.\" \n",
            "Robert's face grew serious. \"I don't want to be one of them,\" he swore at Ned. \"I hate them.\" \n",
            "\"Are you mad at me, Ned?\" \n",
            "Robert let him go. \"No,\" he insisted. \"I hate them.\n",
            "\n",
            "[201 | 500.12] loss=2.53 avg=2.75\n",
            "[202 | 502.49] loss=2.62 avg=2.75\n",
            "[203 | 504.84] loss=2.81 avg=2.75\n",
            "[204 | 507.17] loss=2.72 avg=2.75\n",
            "[205 | 509.52] loss=2.48 avg=2.75\n",
            "[206 | 511.86] loss=2.53 avg=2.75\n",
            "[207 | 514.22] loss=2.62 avg=2.75\n",
            "[208 | 516.57] loss=2.70 avg=2.75\n",
            "[209 | 518.92] loss=2.55 avg=2.74\n",
            "[210 | 521.27] loss=2.38 avg=2.74\n",
            "[211 | 523.62] loss=2.44 avg=2.74\n",
            "[212 | 525.98] loss=2.67 avg=2.74\n",
            "[213 | 528.33] loss=2.50 avg=2.73\n",
            "[214 | 530.68] loss=2.78 avg=2.73\n",
            "[215 | 533.03] loss=2.49 avg=2.73\n",
            "[216 | 535.37] loss=2.39 avg=2.73\n",
            "[217 | 537.73] loss=2.71 avg=2.73\n",
            "[218 | 540.09] loss=2.54 avg=2.72\n",
            "[219 | 542.44] loss=2.43 avg=2.72\n",
            "[220 | 544.80] loss=2.49 avg=2.72\n",
            "[221 | 547.15] loss=2.72 avg=2.72\n",
            "[222 | 549.50] loss=2.36 avg=2.71\n",
            "[223 | 551.87] loss=2.59 avg=2.71\n",
            "[224 | 554.21] loss=2.63 avg=2.71\n",
            "[225 | 556.57] loss=2.43 avg=2.71\n",
            "[226 | 558.91] loss=2.66 avg=2.71\n",
            "[227 | 561.26] loss=2.55 avg=2.71\n",
            "[228 | 563.63] loss=2.36 avg=2.70\n",
            "[229 | 565.98] loss=2.68 avg=2.70\n",
            "[230 | 568.33] loss=2.64 avg=2.70\n",
            "[231 | 570.69] loss=2.42 avg=2.70\n",
            "[232 | 573.04] loss=2.38 avg=2.70\n",
            "[233 | 575.40] loss=2.41 avg=2.69\n",
            "[234 | 577.76] loss=2.52 avg=2.69\n",
            "[235 | 580.12] loss=2.69 avg=2.69\n",
            "[236 | 582.47] loss=2.50 avg=2.69\n",
            "[237 | 584.81] loss=2.52 avg=2.69\n",
            "[238 | 587.17] loss=2.63 avg=2.69\n",
            "[239 | 589.52] loss=2.44 avg=2.68\n",
            "[240 | 591.87] loss=2.52 avg=2.68\n",
            "[241 | 594.23] loss=2.37 avg=2.68\n",
            "[242 | 596.60] loss=2.27 avg=2.67\n",
            "[243 | 598.95] loss=2.34 avg=2.67\n",
            "[244 | 601.30] loss=2.72 avg=2.67\n",
            "[245 | 603.66] loss=2.57 avg=2.67\n",
            "[246 | 606.01] loss=2.47 avg=2.67\n",
            "[247 | 608.36] loss=2.58 avg=2.67\n",
            "[248 | 610.71] loss=2.68 avg=2.67\n",
            "[249 | 613.08] loss=2.39 avg=2.66\n",
            "[250 | 615.43] loss=2.42 avg=2.66\n",
            "[251 | 617.77] loss=2.39 avg=2.66\n",
            "[252 | 620.12] loss=2.22 avg=2.65\n",
            "[253 | 622.47] loss=2.24 avg=2.65\n",
            "[254 | 624.84] loss=2.47 avg=2.65\n",
            "[255 | 627.18] loss=2.33 avg=2.64\n",
            "[256 | 629.53] loss=2.59 avg=2.64\n",
            "[257 | 631.88] loss=2.53 avg=2.64\n",
            "[258 | 634.23] loss=2.38 avg=2.64\n",
            "[259 | 636.58] loss=2.56 avg=2.64\n",
            "[260 | 638.94] loss=2.35 avg=2.63\n",
            "[261 | 641.29] loss=2.67 avg=2.63\n",
            "[262 | 643.63] loss=2.63 avg=2.63\n",
            "[263 | 645.98] loss=2.40 avg=2.63\n",
            "[264 | 648.33] loss=2.37 avg=2.63\n",
            "[265 | 650.69] loss=2.41 avg=2.63\n",
            "[266 | 653.03] loss=2.37 avg=2.62\n",
            "[267 | 655.38] loss=2.29 avg=2.62\n",
            "[268 | 657.72] loss=2.49 avg=2.62\n",
            "[269 | 660.06] loss=2.24 avg=2.62\n",
            "[270 | 662.41] loss=2.38 avg=2.61\n",
            "[271 | 664.76] loss=2.12 avg=2.61\n",
            "[272 | 667.11] loss=2.52 avg=2.61\n",
            "[273 | 669.45] loss=2.50 avg=2.61\n",
            "[274 | 671.81] loss=2.37 avg=2.60\n",
            "[275 | 674.15] loss=2.20 avg=2.60\n",
            "[276 | 676.50] loss=2.44 avg=2.60\n",
            "[277 | 678.84] loss=2.48 avg=2.60\n",
            "[278 | 681.19] loss=2.41 avg=2.59\n",
            "[279 | 683.54] loss=2.27 avg=2.59\n",
            "[280 | 685.90] loss=2.36 avg=2.59\n",
            "[281 | 688.24] loss=2.22 avg=2.58\n",
            "[282 | 690.58] loss=2.45 avg=2.58\n",
            "[283 | 692.93] loss=2.58 avg=2.58\n",
            "[284 | 695.28] loss=2.42 avg=2.58\n",
            "[285 | 697.63] loss=2.30 avg=2.58\n",
            "[286 | 699.98] loss=2.50 avg=2.58\n",
            "[287 | 702.32] loss=2.17 avg=2.57\n",
            "[288 | 704.66] loss=2.16 avg=2.57\n",
            "[289 | 707.01] loss=2.20 avg=2.56\n",
            "[290 | 709.36] loss=2.41 avg=2.56\n",
            "[291 | 711.70] loss=2.58 avg=2.56\n",
            "[292 | 714.05] loss=2.50 avg=2.56\n",
            "[293 | 716.40] loss=2.36 avg=2.56\n",
            "[294 | 718.73] loss=2.46 avg=2.56\n",
            "[295 | 721.09] loss=2.24 avg=2.56\n",
            "[296 | 723.43] loss=2.54 avg=2.56\n",
            "[297 | 725.78] loss=2.41 avg=2.55\n",
            "[298 | 728.13] loss=2.53 avg=2.55\n",
            "[299 | 730.47] loss=2.59 avg=2.55\n",
            "[300 | 732.82] loss=2.34 avg=2.55\n",
            "======== SAMPLE 1 ========\n",
            " He could hear them talking, shouting in the street, with the men at their feet. The first time they called for his help, he was shaking like a baby. \n",
            "\"You did not ask for my help, Lord Eddard,\" a second woman said. \"Get down from this wall and help me save you.\" \n",
            "\"You, damn it,\" the girl said. \n",
            "\"It's a very hard thing to do. I was born in Casterly Rock.\" \n",
            "They stopped, but she had no words. \"You are not to go . . . I'll tell him. I promise you. You and I are the same. \n",
            "I will,\" Stark murmured as they made their exit into the city. \n",
            "He climbed to his feet and began to lift a fist as the woman went to him. She was as slender as his, and she was \n",
            "wet and \n",
            "breathing thin. They exchanged glances. His hand was wrapped tight around his cock with the other. They could feel it \n",
            "pressing hard on his throbbing nipple, a soft sound that made Catelyn realize that he was about to do something. \n",
            "Page 234\n",
            "\n",
            "\"You must not die, Robert,\" she begged him. \"The queen does not care who dies.\" \n",
            "\"She does,\" he said. \n",
            "He could feel her cunt against the wall, as if a hand were grasping for her, and he heard his brother speaking. \n",
            "The girl's face was red, her hair parted, her face puffy, her eyes heavy with lust. \n",
            "He felt her throbbing, and his legs ached on his knees. \"Robert?\" \n",
            "She did not answer, but for her, someone to hear, someone he did not know. She was a girl, and he knew she \n",
            "am of Stark and Lannister, and it would not be right for him to betray her. \"Do it, my lord, I will.\" \n",
            "Then he took out his sword, the long staff that he had held for Jon as he rode. He was certain he \n",
            "got a pretty sword. Jon was so young and slender he looked like the cut-off foot of a sheep. His brother was \n",
            "the biggest boy in the castle. He lifted his hand to clasp the girl as he strode to her. \"You want this?\" \n",
            "He heard her call out and took a long breath. \"I don't want anything,\" he said. \"I don't want the sword. \n",
            "It will kill you if you stab him. Please, Ned, do as I will.\" \n",
            "The girl giggled. \"You will do as I will, no matter what, but it's a cruel word. There's only one way \n",
            "to know. Go back to the Tower of the Hand with me. Do whatever it takes. I promise you, if you tell me the words, \n",
            "my father will laugh at you and run off to hell.\" She was red in his arms, pale with rage. \n",
            "\"I will,\" he repeated. \"I promise, I swear, no one can fight for you. If I find you there, I'll kill you, you \n",
            "bastard.\" \n",
            "The girl squirmed away and moved away, giggling again. It was madness. The words might have helped her, but she \n",
            "would never do it, and if she cried, damn it, she would cry every day. \n",
            "As they passed a large room and a pair of huge grey boys, Robert Stark looked at her strangely. His eyes \n",
            "lifted out and turned. Her brother Jon gave a shuddering cry. \"I'm sure it will be quite a story, Lord Eddard. A \n",
            "double tale.\" \n",
            "\"You cannot let these two fight each other. Don't they take you for a brother?\" He turned around. \n",
            "It took a few moments, yet they were still as hard as he had feared they would be, as fierce warriors. He could feel something \n",
            "of rage at the sight of his brother in this strange position. \"Your father's an old man, Robert, he's a blood brother \n",
            "of Aegon. Your father's a godswood man. I can't let these two fight, let alone be one.\" \n",
            "The man on the other side of the room stared at him, a cold glint in his eyes. \"They can't even-\" \n",
            "He looked in the face, deep inside his bones, and knew. Jon had known for years, when it came to his \n",
            "father. He could recall Jon's face. \"Your uncle Ned,\" he said, \"has died, and you went off to serve here, just \n",
            "for a few days, you can't remember his name.\" \n",
            "There was some blood on the boy's cheek, and he wrenched one hand back and forth as\n",
            "\n",
            "[301 | 746.16] loss=2.14 avg=2.55\n",
            "[302 | 748.50] loss=2.58 avg=2.55\n",
            "[303 | 750.85] loss=2.31 avg=2.55\n",
            "[304 | 753.20] loss=2.43 avg=2.54\n",
            "[305 | 755.54] loss=2.28 avg=2.54\n",
            "[306 | 757.89] loss=2.40 avg=2.54\n",
            "[307 | 760.24] loss=2.24 avg=2.54\n",
            "[308 | 762.59] loss=2.15 avg=2.53\n",
            "[309 | 764.94] loss=2.27 avg=2.53\n",
            "[310 | 767.28] loss=2.46 avg=2.53\n",
            "[311 | 769.62] loss=2.35 avg=2.53\n",
            "[312 | 771.97] loss=2.46 avg=2.53\n",
            "[313 | 774.32] loss=2.35 avg=2.52\n",
            "[314 | 776.66] loss=2.24 avg=2.52\n",
            "[315 | 779.01] loss=2.08 avg=2.52\n",
            "[316 | 781.36] loss=2.34 avg=2.52\n",
            "[317 | 783.71] loss=2.37 avg=2.51\n",
            "[318 | 786.04] loss=2.20 avg=2.51\n",
            "[319 | 788.39] loss=2.05 avg=2.51\n",
            "[320 | 790.73] loss=2.32 avg=2.50\n",
            "[321 | 793.08] loss=2.32 avg=2.50\n",
            "[322 | 795.43] loss=2.16 avg=2.50\n",
            "[323 | 797.78] loss=2.31 avg=2.50\n",
            "[324 | 800.13] loss=2.24 avg=2.49\n",
            "[325 | 802.49] loss=2.36 avg=2.49\n",
            "[326 | 804.83] loss=2.26 avg=2.49\n",
            "[327 | 807.18] loss=2.42 avg=2.49\n",
            "[328 | 809.53] loss=2.41 avg=2.49\n",
            "[329 | 811.88] loss=2.16 avg=2.48\n",
            "[330 | 814.22] loss=2.42 avg=2.48\n",
            "[331 | 816.57] loss=2.31 avg=2.48\n",
            "[332 | 818.92] loss=2.15 avg=2.48\n",
            "[333 | 821.27] loss=2.29 avg=2.48\n",
            "[334 | 823.62] loss=2.59 avg=2.48\n",
            "[335 | 825.96] loss=2.24 avg=2.48\n",
            "[336 | 828.31] loss=2.28 avg=2.47\n",
            "[337 | 830.66] loss=2.20 avg=2.47\n",
            "[338 | 833.02] loss=2.27 avg=2.47\n",
            "[339 | 835.36] loss=2.12 avg=2.46\n",
            "[340 | 837.71] loss=2.20 avg=2.46\n",
            "[341 | 840.05] loss=2.32 avg=2.46\n",
            "[342 | 842.40] loss=2.22 avg=2.46\n",
            "[343 | 844.75] loss=2.20 avg=2.46\n",
            "[344 | 847.10] loss=2.06 avg=2.45\n",
            "[345 | 849.45] loss=1.87 avg=2.45\n",
            "[346 | 851.80] loss=2.08 avg=2.44\n",
            "[347 | 854.15] loss=2.35 avg=2.44\n",
            "[348 | 856.50] loss=2.06 avg=2.44\n",
            "[349 | 858.85] loss=2.03 avg=2.43\n",
            "[350 | 861.20] loss=2.09 avg=2.43\n",
            "[351 | 863.55] loss=2.23 avg=2.43\n",
            "[352 | 865.90] loss=2.29 avg=2.43\n",
            "[353 | 868.27] loss=2.27 avg=2.42\n",
            "[354 | 870.62] loss=2.17 avg=2.42\n",
            "[355 | 872.98] loss=2.23 avg=2.42\n",
            "[356 | 875.34] loss=2.11 avg=2.42\n",
            "[357 | 877.68] loss=2.14 avg=2.41\n",
            "[358 | 880.05] loss=1.94 avg=2.41\n",
            "[359 | 882.41] loss=2.34 avg=2.41\n",
            "[360 | 884.77] loss=2.35 avg=2.41\n",
            "[361 | 887.12] loss=2.21 avg=2.41\n",
            "[362 | 889.47] loss=2.43 avg=2.41\n",
            "[363 | 891.83] loss=2.31 avg=2.40\n",
            "[364 | 894.18] loss=1.80 avg=2.40\n",
            "[365 | 896.53] loss=2.32 avg=2.40\n",
            "[366 | 898.89] loss=2.36 avg=2.40\n",
            "[367 | 901.27] loss=2.23 avg=2.40\n",
            "[368 | 903.62] loss=2.06 avg=2.39\n",
            "[369 | 905.98] loss=2.27 avg=2.39\n",
            "[370 | 908.33] loss=2.22 avg=2.39\n",
            "[371 | 910.68] loss=2.10 avg=2.39\n",
            "[372 | 913.03] loss=2.53 avg=2.39\n",
            "[373 | 915.38] loss=2.16 avg=2.39\n",
            "[374 | 917.74] loss=2.17 avg=2.38\n",
            "[375 | 920.10] loss=2.11 avg=2.38\n",
            "[376 | 922.45] loss=2.26 avg=2.38\n",
            "[377 | 924.80] loss=2.17 avg=2.38\n",
            "[378 | 927.16] loss=2.15 avg=2.37\n",
            "[379 | 929.50] loss=2.24 avg=2.37\n",
            "[380 | 931.86] loss=2.04 avg=2.37\n",
            "[381 | 934.21] loss=2.06 avg=2.37\n",
            "[382 | 936.56] loss=2.08 avg=2.36\n",
            "[383 | 938.90] loss=2.11 avg=2.36\n",
            "[384 | 941.25] loss=2.13 avg=2.36\n",
            "[385 | 943.60] loss=1.96 avg=2.35\n",
            "[386 | 945.97] loss=2.16 avg=2.35\n",
            "[387 | 948.32] loss=1.88 avg=2.35\n",
            "[388 | 950.67] loss=2.07 avg=2.35\n",
            "[389 | 953.02] loss=1.78 avg=2.34\n",
            "[390 | 955.36] loss=1.87 avg=2.33\n",
            "[391 | 957.71] loss=2.11 avg=2.33\n",
            "[392 | 960.06] loss=2.17 avg=2.33\n",
            "[393 | 962.40] loss=2.03 avg=2.33\n",
            "[394 | 964.75] loss=2.36 avg=2.33\n",
            "[395 | 967.12] loss=2.29 avg=2.33\n",
            "[396 | 969.47] loss=1.89 avg=2.32\n",
            "[397 | 971.82] loss=2.29 avg=2.32\n",
            "[398 | 974.17] loss=2.27 avg=2.32\n",
            "[399 | 976.52] loss=2.14 avg=2.32\n",
            "[400 | 978.87] loss=2.35 avg=2.32\n",
            "======== SAMPLE 1 ========\n",
            " \n",
            "as you have done, my lady.\" \n",
            "\"Oh, and there's a secret cellar,\" Ser Rodrik said as he slid the chain back into place and \n",
            "carried it toward the door. \"A vault?\" \n",
            "\"A cellar, lady,\" Ser Rodrik agreed. He slid the knife across his belt and knotted the \n",
            "blade close \n",
            "around his throat. A moment later an urgent warning went up from the man beside Jon. \"The Wall,\" he \n",
            "declared, \"is my home. I have a watch.\" He took another step toward the door, his breath steaming, and \n",
            "dysfunctional. \"I am to carry your swords.\" \n",
            "\"You are a maester of the \n",
            "side with the kings,\" Ser Rodrik answered. \"I shall have the logs.\" \n",
            "The woman looked at him. \"Did you make a note in your papers of where I should mark your departure?\" \n",
            "The boy looked from Ser Rodrik to the woman. \"I should like to see my father,\" he said. \n",
            "Page 25\n",
            "\n",
            "\"My father,\" Ser Rodrik said. \"What should I do with him?\" \n",
            "\"Go,\" she said. She threw off her belt to help bring her purse up. Her little monkey son, she thought. \n",
            "\"Go.\" He led her over to where the guardsmen stood before the door. They leaned backward on their \n",
            "couchets as he led her off, but not before Ser Rodrik turned to follow. He seemed to hear a \n",
            "word, then saw the exit. \n",
            "A moment later as three guardsmen in silvery plate mail arrived \n",
            "behind him, the man who led them turned on them. \"My lord,\" he said. \"Are you afraid?\" As the man spoke \n",
            "word after word about his daughter and the boy who followed, the door opened. The guardsmen rose \n",
            "and looked down on them. \n",
            "\"Come in, Ser Rodrik,\" the woman said. \"Where's your guard last?\" \n",
            "\"My lady,\" Ser Rodrik said, giving her his seal a quick tap. \"I am very familiar with the guards, do you know \n",
            "what I'm about to say?\" \n",
            "The woman opened the door, a delicate piece of cloth wrapped tightly in silk, and led him to \n",
            "his seat. It was the only room in the castle where the queen did not have a guard . . . \n",
            "\"Your father will need to bring me an urgent letter from the maester,\" the woman said. \n",
            "\"He will need to present himself,\" Ser Rodrik answered. \n",
            "\"The letter,\" she said. \"Send it along with the rest of these maps. I will be glad to help.\" \n",
            "Ser Rodrik made no move. He was about to tell the lady at the door when he heard the woman's \n",
            "voice. \"My lord, a letter,\" he said. \"The lord speaks in his own tongue. That is very pleasing, my lady. Is the \n",
            "letter a map?\" He could not wait for her to make the final call. \n",
            "Ser Rodrik knelt on his chair, and took his paper in hand, but it was too late. She had left the \n",
            "Page 26\n",
            "\n",
            "message already, with all the rest of that news of a daughter or a boy . . . well, not even her, she was so \n",
            "familiar with the rest of things, so he did not have to hear about it. He set off to make haste. \"My lady,\" \n",
            "he was about to tell her he was leaving. \"I hope I won't have to wait long. Your lord father is to go to the \n",
            "Wall by sunrise on the last day. I have no need to leave you late.\" \n",
            "She was still dressed in silk when she said \"my lady.\" \n",
            "\"My lord.\" \n",
            "\"My lady,\" Ser Rodrik said. \"Tell him the truth.\" \n",
            "Ser Rodrik's legs were already so stiff, he had no weight left over him. \"My lady, my hands were \n",
            "sore, they had to stop for certain. No food. I had to eat from table, I guess, but it was \n",
            "too very cold. Too cold. And the girl must do with drinking.\" \n",
            "\"Drink,\" Ser Rodrik told her. His fingers twitched nervously. \"I've been drinking, my lady. I want \n",
            "to be alone tonight.\" \n",
            "\"You do not understand, Ser Rodrik, please, do not say I cannot.\" \n",
            "\"No,\" she said. \"I cannot leave this castle alone. My lady . . . leave this castle. You must see \n",
            "the Wall, no matter what it might cost you. If you're too weary to go, you might even be glad to see the \n",
            "library\n",
            "\n",
            "[401 | 992.08] loss=2.36 avg=2.32\n",
            "[402 | 994.43] loss=2.17 avg=2.32\n",
            "[403 | 996.78] loss=2.19 avg=2.32\n",
            "[404 | 999.13] loss=2.11 avg=2.32\n",
            "[405 | 1001.48] loss=1.85 avg=2.31\n",
            "[406 | 1003.84] loss=1.78 avg=2.31\n",
            "[407 | 1006.18] loss=2.17 avg=2.30\n",
            "[408 | 1008.53] loss=1.64 avg=2.30\n",
            "[409 | 1010.88] loss=1.96 avg=2.29\n",
            "[410 | 1013.21] loss=2.08 avg=2.29\n",
            "[411 | 1015.56] loss=2.13 avg=2.29\n",
            "[412 | 1017.91] loss=1.92 avg=2.29\n",
            "[413 | 1020.25] loss=2.10 avg=2.28\n",
            "[414 | 1022.59] loss=2.08 avg=2.28\n",
            "[415 | 1024.93] loss=2.16 avg=2.28\n",
            "[416 | 1027.28] loss=1.98 avg=2.28\n",
            "[417 | 1029.63] loss=1.92 avg=2.27\n",
            "[418 | 1031.96] loss=1.91 avg=2.27\n",
            "[419 | 1034.30] loss=2.19 avg=2.27\n",
            "[420 | 1036.65] loss=1.96 avg=2.27\n",
            "[421 | 1039.01] loss=1.98 avg=2.26\n",
            "[422 | 1041.35] loss=1.84 avg=2.26\n",
            "[423 | 1043.70] loss=2.00 avg=2.26\n",
            "[424 | 1046.04] loss=2.19 avg=2.26\n",
            "[425 | 1048.39] loss=2.06 avg=2.25\n",
            "[426 | 1050.74] loss=2.01 avg=2.25\n",
            "[427 | 1053.09] loss=2.08 avg=2.25\n",
            "[428 | 1055.44] loss=2.08 avg=2.25\n",
            "[429 | 1057.79] loss=1.96 avg=2.25\n",
            "[430 | 1060.14] loss=2.13 avg=2.24\n",
            "[431 | 1062.50] loss=2.06 avg=2.24\n",
            "[432 | 1064.85] loss=2.05 avg=2.24\n",
            "[433 | 1067.18] loss=2.05 avg=2.24\n",
            "[434 | 1069.54] loss=2.01 avg=2.24\n",
            "[435 | 1071.89] loss=1.94 avg=2.23\n",
            "[436 | 1074.24] loss=1.97 avg=2.23\n",
            "[437 | 1076.60] loss=1.76 avg=2.23\n",
            "[438 | 1078.95] loss=1.91 avg=2.22\n",
            "[439 | 1081.30] loss=2.12 avg=2.22\n",
            "[440 | 1083.64] loss=2.07 avg=2.22\n",
            "[441 | 1085.99] loss=1.97 avg=2.22\n",
            "[442 | 1088.34] loss=2.26 avg=2.22\n",
            "[443 | 1090.69] loss=2.20 avg=2.22\n",
            "[444 | 1093.04] loss=1.93 avg=2.21\n",
            "[445 | 1095.39] loss=1.93 avg=2.21\n",
            "[446 | 1097.74] loss=1.92 avg=2.21\n",
            "[447 | 1100.09] loss=2.11 avg=2.21\n",
            "[448 | 1102.44] loss=1.82 avg=2.20\n",
            "[449 | 1104.79] loss=1.88 avg=2.20\n",
            "[450 | 1107.14] loss=1.98 avg=2.20\n",
            "[451 | 1109.50] loss=1.74 avg=2.19\n",
            "[452 | 1111.87] loss=1.93 avg=2.19\n",
            "[453 | 1114.22] loss=2.20 avg=2.19\n",
            "[454 | 1116.57] loss=1.82 avg=2.19\n",
            "[455 | 1118.92] loss=1.83 avg=2.18\n",
            "[456 | 1121.27] loss=1.98 avg=2.18\n",
            "[457 | 1123.62] loss=1.75 avg=2.18\n",
            "[458 | 1125.97] loss=2.06 avg=2.18\n",
            "[459 | 1128.31] loss=1.72 avg=2.17\n",
            "[460 | 1130.67] loss=1.99 avg=2.17\n",
            "[461 | 1133.02] loss=2.08 avg=2.17\n",
            "[462 | 1135.37] loss=2.07 avg=2.17\n",
            "[463 | 1137.72] loss=1.85 avg=2.16\n",
            "[464 | 1140.07] loss=1.73 avg=2.16\n",
            "[465 | 1142.42] loss=1.79 avg=2.16\n",
            "[466 | 1144.77] loss=1.92 avg=2.15\n",
            "[467 | 1147.12] loss=1.74 avg=2.15\n",
            "[468 | 1149.47] loss=1.69 avg=2.15\n",
            "[469 | 1151.83] loss=1.63 avg=2.14\n",
            "[470 | 1154.18] loss=1.76 avg=2.14\n",
            "[471 | 1156.53] loss=1.79 avg=2.13\n",
            "[472 | 1158.87] loss=2.19 avg=2.13\n",
            "[473 | 1161.23] loss=1.66 avg=2.13\n",
            "[474 | 1163.57] loss=2.04 avg=2.13\n",
            "[475 | 1165.92] loss=2.27 avg=2.13\n",
            "[476 | 1168.27] loss=1.68 avg=2.12\n",
            "[477 | 1170.62] loss=1.62 avg=2.12\n",
            "[478 | 1172.97] loss=1.72 avg=2.12\n",
            "[479 | 1175.31] loss=1.58 avg=2.11\n",
            "[480 | 1177.66] loss=2.11 avg=2.11\n",
            "[481 | 1180.01] loss=1.92 avg=2.11\n",
            "[482 | 1182.36] loss=1.93 avg=2.11\n",
            "[483 | 1184.70] loss=1.80 avg=2.10\n",
            "[484 | 1187.05] loss=2.06 avg=2.10\n",
            "[485 | 1189.40] loss=1.78 avg=2.10\n",
            "[486 | 1191.75] loss=1.60 avg=2.09\n",
            "[487 | 1194.10] loss=1.61 avg=2.09\n",
            "[488 | 1196.46] loss=1.88 avg=2.09\n",
            "[489 | 1198.80] loss=1.96 avg=2.09\n",
            "[490 | 1201.14] loss=1.59 avg=2.08\n",
            "[491 | 1203.49] loss=1.60 avg=2.08\n",
            "[492 | 1205.84] loss=1.74 avg=2.07\n",
            "[493 | 1208.17] loss=1.98 avg=2.07\n",
            "[494 | 1210.52] loss=1.73 avg=2.07\n",
            "[495 | 1212.86] loss=1.73 avg=2.07\n",
            "[496 | 1215.21] loss=1.83 avg=2.06\n",
            "[497 | 1217.56] loss=1.78 avg=2.06\n",
            "[498 | 1219.90] loss=1.23 avg=2.05\n",
            "[499 | 1222.25] loss=1.56 avg=2.05\n",
            "[500 | 1224.60] loss=1.65 avg=2.04\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92aee661-6457-42bf-e1b5-eab362cf8e04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? \n",
            "Tyrion's face was pale and confused. \n",
            "\"I can't go back in time,\" Ser Vardis said. \"The Old Gods have determined that the first world is a dead \n",
            "one, and that the children of man must go on living.\" \n",
            "\"I know,\" Tyrion said. \"I can't go back in time.\" \n",
            "\"You cannot,\" Ser Vardis said. \"There is no one else on the face of the earth.\" \n",
            "\"And if there is no one else on the face of the earth?\" Tyrion asked. \n",
            "\"There are no more than ten thousand light years away.\" \n",
            "\"And the sun of your generation is gone,\" Ser V\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "5c9712fa-a38c-45d9-fd7f-36d4000b0052"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}